{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e972651",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sat Dec 24 12:22:36 2022\n",
    "\n",
    "@author: eniseranabeklen\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from selenium import webdriver\n",
    "from shutil import which\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException, ElementNotInteractableException\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# fetch_jobs functions takes keyword, num_pages, and location as arguments. The function opens Glassdoor on Google Chrome and uses its arguments to search jobs.\n",
    "\n",
    "def fetch_jobs(keyword, num_pages, location):\n",
    "    options = Options()\n",
    "    options.add_argument(\"window-size=1920,1080\")\n",
    "    #Enter your chromedriver.exe path below\n",
    "    chrome_path = r\"C/Users/eniseranabeklen/Documents/GitHub/ds_salary_proj/chromedriver.exe\"\n",
    "    driver = webdriver.Chrome(executable_path=chrome_path, options=options)\n",
    "    driver.get(\"https://www.glassdoor.com/Job/index.htm\")\n",
    "    search_input = driver.find_element(By.ID, \"KeywordSearch\")\n",
    "    location_input = driver.find_element(By.ID, \"LocationSearch\")\n",
    "    location_input.clear()\n",
    "    search_input.send_keys(keyword)\n",
    "    location_input.send_keys(location)\n",
    "    driver.find_element(By.ID, \"HeroSearchButton\").click()\n",
    "    time.sleep(2)\n",
    "    \n",
    "# we are creating lists to save the scraped data.     \n",
    "    company_name = []\n",
    "    job_title = []\n",
    "    salary_est = []\n",
    "    location = []\n",
    "    job_description = []\n",
    "    salary_estimate = []\n",
    "    company_size = []\n",
    "    company_type = []\n",
    "    company_sector = []\n",
    "    company_industry = []\n",
    "    company_founded = []\n",
    "    company_revenue = []\n",
    "    \n",
    "    \n",
    "    #Set current page to 1\n",
    "    current_page = 1     \n",
    "    time.sleep(3)\n",
    "    \n",
    "    while current_page <= num_pages:   \n",
    "        \n",
    "        done = False\n",
    "        count = 0 \n",
    "        while not done:\n",
    "            #job_cards = driver.find_elements(By.XPATH, \"//article[@id='MainCol']//ul/li[@data-adv-type='GENERAL']\")\n",
    "            \n",
    "            job_cards = driver.find_elements(By.XPATH, \"//li[@data-adv-type='GENERAL']\")\n",
    "            \n",
    "            for card in job_cards:\n",
    "                card.click()\n",
    "                time.sleep(3)\n",
    "                count += 1\n",
    "                #Closes the signup prompt\n",
    "                try:\n",
    "                    driver.find_element(By.XPATH,\".//span[@class='SVGInline modal_closeIcon']\").click()\n",
    "                    time.sleep(2)\n",
    "                except NoSuchElementException:\n",
    "                    time.sleep(2)\n",
    "                    pass\n",
    "\n",
    "                #Expands the Description section by clicking on Show More\n",
    "                try:\n",
    "                    driver.find_element(By.XPATH,\"//*[@id='JobDescriptionContainer']/div[2]\").click()\n",
    "                    time.sleep(1)\n",
    "                except NoSuchElementException:\n",
    "                    card.click()\n",
    "                    print(str(current_page) + '#ERROR: no such element')\n",
    "                    driver.refresh()\n",
    "                    pass\n",
    "                except ElementNotInteractableException:\n",
    "                    card.click()\n",
    "                    driver.implicitly_wait(30)\n",
    "                    driver.refresh()\n",
    "                    print(str(current_page) + '#ERROR: not interactable')\n",
    "                    pass\n",
    "\n",
    "                #Scrape \n",
    "                try:\n",
    "                    company_name.append(driver.find_element(By.XPATH,\"//*[@id='JDCol']/div/article/div/div[1]/div/div/div/div/div[1]/div[1]/div\").text)\n",
    "                except:\n",
    "                    company_name.append(\"#N/A\")\n",
    "                    pass\n",
    "\n",
    "                try:\n",
    "                    job_title.append(driver.find_element(By.XPATH,'//*[@id=\"JDCol\"]/div/article/div/div[1]/div/div/div/div/div[1]/div[2]').text)\n",
    "                except:\n",
    "                    job_title.append(\"#N/A\")\n",
    "                    pass\n",
    "\n",
    "                try:\n",
    "                    location.append(driver.find_element(By.XPATH,'//*[@id=\"JDCol\"]/div/article/div/div[1]/div/div/div[1]/div[3]/div[1]/div[3]').text)\n",
    "                except:\n",
    "                    location.append(\"#N/A\")\n",
    "                    pass\n",
    "\n",
    "                try:\n",
    "                    job_description.append(driver.find_element(By.XPATH,\"//div[@id='JobDescriptionContainer']\").text)\n",
    "                except:\n",
    "                    job_description.append(\"#N/A\")\n",
    "                    pass\n",
    "                \n",
    "                #we need to iterate through job listings to extract salary estimate data\n",
    "                xpath = '//*[@id=\"MainCol\"]/div[1]/ul/li[' + str(count) + ']/div[2]/div[3]/div[1]/span'\n",
    "                try:\n",
    "                    salary_estimate.append(driver.find_element(By.XPATH, xpath).text)\n",
    "                except:\n",
    "                    salary_estimate.append(\"#N/A\")\n",
    "                    pass\n",
    "                \n",
    "                try:\n",
    "                    company_size.append(driver.find_element(By.XPATH,\"//div[@id='CompanyContainer']//span[text()='Size']//following-sibling::*\").text)\n",
    "                except:\n",
    "                    company_size.append(\"#N/A\")\n",
    "                    pass\n",
    "                \n",
    "                try:\n",
    "                    company_type.append(driver.find_element(By.XPATH,\"//div[@id='CompanyContainer']//span[text()='Type']//following-sibling::*\").text)\n",
    "                except:\n",
    "                    company_type.append(\"#N/A\")\n",
    "                    pass\n",
    "                    \n",
    "                try:\n",
    "                    company_sector.append(driver.find_element(By.XPATH,\"//div[@id='CompanyContainer']//span[text()='Sector']//following-sibling::*\").text)\n",
    "                except:\n",
    "                    company_sector.append(\"#N/A\")\n",
    "                    pass\n",
    "                    \n",
    "                try:\n",
    "                    company_industry.append(driver.find_element(By.XPATH,\"//div[@id='CompanyContainer']//span[text()='Industry']//following-sibling::*\").text)\n",
    "                except:\n",
    "                    company_industry.append(\"#N/A\")\n",
    "                    pass\n",
    "                     \n",
    "                try:\n",
    "                    company_founded.append(driver.find_element(By.XPATH,\"//div[@id='CompanyContainer']//span[text()='Founded']//following-sibling::*\").text)\n",
    "                except:\n",
    "                    company_founded.append(\"#N/A\")\n",
    "                    pass\n",
    "                    \n",
    "                try:\n",
    "                    company_revenue.append(driver.find_element(By.XPATH,\"//div[@id='CompanyContainer']//span[text()='Revenue']//following-sibling::*\").text)\n",
    "                except:\n",
    "                    company_revenue.append(\"#N/A\")\n",
    "                    pass\n",
    "\n",
    "                    \n",
    "                done = True\n",
    "                \n",
    "       # Moves to the next page         \n",
    "        if done:\n",
    "            print(str(current_page) + ' ' + 'out of' +' '+ str(num_pages) + ' ' + 'pages done')\n",
    "            driver.find_element(By.XPATH,\"//span[@alt='next-icon']\").click()   \n",
    "            current_page = current_page + 1\n",
    "            time.sleep(4)\n",
    "\n",
    "\n",
    "    driver.close()\n",
    "    df = pd.DataFrame({'company': company_name, \n",
    "        'job title': job_title,\n",
    "        'location': location,\n",
    "        'job description': job_description,\n",
    "        'salary estimate': salary_estimate,\n",
    "        'company_size': company_size,\n",
    "        'company_type': company_type,\n",
    "        'company_sector': company_sector,\n",
    "        'company_industry' : company_industry, \n",
    "        'company_founded' : company_founded, \n",
    "        'company_revenue': company_revenue})\n",
    "    \n",
    "\n",
    "    df.to_csv(keyword + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b13e02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sat Dec 24 12:22:36 2022\n",
    "\n",
    "@author: eniseranabeklen\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from selenium import webdriver\n",
    "from shutil import which\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException, ElementNotInteractableException\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# fetch_jobs functions takes keyword, num_pages, and location as arguments. The function opens Glassdoor on Google Chrome and uses its arguments to search jobs.\n",
    "\n",
    "def fetch_jobs(keyword, num_pages, location):\n",
    "    options = Options()\n",
    "    options.add_argument(\"window-size=1920,1080\")\n",
    "    driver = webdriver.Chrome(options=options)  # auto uses ChromeDriver if installed\n",
    "    driver.get(\"https://www.glassdoor.com/Job/index.html\")\n",
    "    \n",
    "    search_input = driver.find_element(By.ID, \"KeywordSearch\")\n",
    "    location_input = driver.find_element(By.ID, \"LocationSearch\")\n",
    "    location_input.clear()\n",
    "    search_input.send_keys(keyword)\n",
    "    location_input.send_keys(location)\n",
    "    driver.find_element(By.ID, \"HeroSearchButton\").click()\n",
    "    time.sleep(3)\n",
    "\n",
    "    # Initialize lists\n",
    "    company_name, job_title, location_list, job_description, salary_estimate = [], [], [], [], []\n",
    "    company_size, company_type, company_sector, company_industry, company_founded, company_revenue = [], [], [], [], [], []\n",
    "    \n",
    "    current_page = 1     \n",
    "    while current_page <= num_pages:   \n",
    "        job_cards = driver.find_elements(By.XPATH, \"//li[@data-adv-type='GENERAL']\")\n",
    "        \n",
    "        for idx, card in enumerate(job_cards, start=1):\n",
    "            try:\n",
    "                card.click()\n",
    "                time.sleep(2)\n",
    "\n",
    "                # Close popups if they appear\n",
    "                try:\n",
    "                    driver.find_element(By.XPATH,\".//span[@class='SVGInline modal_closeIcon']\").click()\n",
    "                    time.sleep(1)\n",
    "                except NoSuchElementException:\n",
    "                    pass\n",
    "\n",
    "                # Scraping\n",
    "                try:\n",
    "                    company_name.append(driver.find_element(By.XPATH,\"//*[@id='JDCol']//div[1]//div[1]//div[1]\").text)\n",
    "                except: company_name.append(\"#N/A\")\n",
    "\n",
    "                try:\n",
    "                    job_title.append(driver.find_element(By.XPATH,'//*[@id=\"JDCol\"]//div[2]').text)\n",
    "                except: job_title.append(\"#N/A\")\n",
    "\n",
    "                try:\n",
    "                    location_list.append(driver.find_element(By.XPATH,'//*[@id=\"JDCol\"]//div[3]').text)\n",
    "                except: location_list.append(\"#N/A\")\n",
    "\n",
    "                try:\n",
    "                    job_description.append(driver.find_element(By.ID,\"JobDescriptionContainer\").text)\n",
    "                except: job_description.append(\"#N/A\")\n",
    "\n",
    "                try:\n",
    "                    salary_estimate.append(driver.find_element(By.XPATH, f'//*[@id=\"MainCol\"]/div[1]/ul/li[{idx}]/div[2]/div[3]/div[1]/span').text)\n",
    "                except: salary_estimate.append(\"#N/A\")\n",
    "\n",
    "                # Company info\n",
    "                try: company_size.append(driver.find_element(By.XPATH,\"//div[@id='CompanyContainer']//span[text()='Size']//following-sibling::*\").text)\n",
    "                except: company_size.append(\"#N/A\")\n",
    "                try: company_type.append(driver.find_element(By.XPATH,\"//div[@id='CompanyContainer']//span[text()='Type']//following-sibling::*\").text)\n",
    "                except: company_type.append(\"#N/A\")\n",
    "                try: company_sector.append(driver.find_element(By.XPATH,\"//div[@id='CompanyContainer']//span[text()='Sector']//following-sibling::*\").text)\n",
    "                except: company_sector.append(\"#N/A\")\n",
    "                try: company_industry.append(driver.find_element(By.XPATH,\"//div[@id='CompanyContainer']//span[text()='Industry']//following-sibling::*\").text)\n",
    "                except: company_industry.append(\"#N/A\")\n",
    "                try: company_founded.append(driver.find_element(By.XPATH,\"//div[@id='CompanyContainer']//span[text()='Founded']//following-sibling::*\").text)\n",
    "                except: company_founded.append(\"#N/A\")\n",
    "                try: company_revenue.append(driver.find_element(By.XPATH,\"//div[@id='CompanyContainer']//span[text()='Revenue']//following-sibling::*\").text)\n",
    "                except: company_revenue.append(\"#N/A\")\n",
    "\n",
    "                # Print each scraped job\n",
    "                print(f\"✅ {job_title[-1]} at {company_name[-1]} ({location_list[-1]})\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\"Error scraping job:\", e)\n",
    "\n",
    "        # Go to next page\n",
    "        try:\n",
    "            next_button = driver.find_element(By.XPATH, \"//button[@aria-label='Next']\")\n",
    "            next_button.click()\n",
    "            current_page += 1\n",
    "            time.sleep(4)\n",
    "        except NoSuchElementException:\n",
    "            break\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'company': company_name, \n",
    "        'job title': job_title,\n",
    "        'location': location_list,\n",
    "        'job description': job_description,\n",
    "        'salary estimate': salary_estimate,\n",
    "        'company size': company_size,\n",
    "        'company type': company_type,\n",
    "        'company sector': company_sector,\n",
    "        'company industry': company_industry,\n",
    "        'company founded': company_founded,\n",
    "        'company revenue': company_revenue\n",
    "    })\n",
    "\n",
    "    print(df.head())\n",
    "    df.to_csv(f\"{keyword}.csv\", index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49abf7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca606823",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "def get_jobs(keyword, num_jobs, verbose, path, slp_time):\n",
    "    '''Gathers jobs as a dataframe, scraped from Glassdoor'''\n",
    "\n",
    "    # Initializing the webdriver\n",
    "    options = webdriver.ChromeOptions()\n",
    "\n",
    "    # Uncomment the line below if you'd like to scrape without a new Chrome window every time.\n",
    "    # options.add_argument('headless')\n",
    "\n",
    "    # FIX: Use Service instead of executable_path\n",
    "    service = Service(path)\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    driver.set_window_size(1120, 1000)\n",
    "\n",
    "\n",
    "    url = \"https://www.glassdoor.com/Job/jobs.htm?suggestCount=0&suggestChosen=false&clickSource=searchBtn&typedKeyword=\"+keyword+\"&sc.keyword=\"+keyword+\"&locT=&locId=&jobType=\"\n",
    "    #url = 'https://www.glassdoor.com/Job/jobs.htm?sc.keyword=\"' + keyword + '\"&locT=C&locId=1147401&locKeyword=San%20Francisco,%20CA&jobType=all&fromAge=-1&minSalary=0&includeNoSalaryJobs=true&radius=100&cityId=-1&minRating=0.0&industryId=-1&sgocId=-1&seniorityType=all&companyId=-1&employerSizes=0&applicationType=0&remoteWorkType=0'\n",
    "    driver.get(url)\n",
    "    jobs = []\n",
    "\n",
    "    while len(jobs) < num_jobs:  #If true, should be still looking for new jobs.\n",
    "\n",
    "        #Let the page load. Change this number based on your internet speed.\n",
    "        #Or, wait until the webpage is loaded, instead of hardcoding it.\n",
    "        time.sleep(slp_time)\n",
    "\n",
    "        #Test for the \"Sign Up\" prompt and get rid of it.\n",
    "        try:\n",
    "            driver.find_element_by_class_name(\"selected\").click()\n",
    "        except ElementClickInterceptedException:\n",
    "            pass\n",
    "\n",
    "        time.sleep(.1)\n",
    "\n",
    "        try:\n",
    "            driver.find_element_by_css_selector('[alt=\"Close\"]').click() #clicking to the X.\n",
    "            print(' x out worked')\n",
    "        except NoSuchElementException:\n",
    "            print(' x out failed')\n",
    "            pass\n",
    "\n",
    "\n",
    "        #Going through each job in this page\n",
    "        job_buttons = driver.find_elements_by_class_name(\"jl\")  #jl for Job Listing. These are the buttons we're going to click.\n",
    "        for job_button in job_buttons:  \n",
    "\n",
    "            print(\"Progress: {}\".format(\"\" + str(len(jobs)) + \"/\" + str(num_jobs)))\n",
    "            if len(jobs) >= num_jobs:\n",
    "                break\n",
    "            try:\n",
    "                job_button.click()  #You might \n",
    "                time.sleep(1)\n",
    "                collected_successfully = False\n",
    "            except:\n",
    "                continue\n",
    "            while not collected_successfully:\n",
    "                try:\n",
    "                    company_name = driver.find_element_by_xpath('.//div[@class=\"employerName\"]').text\n",
    "                    location = driver.find_element_by_xpath('.//div[@class=\"location\"]').text\n",
    "                    job_title = driver.find_element_by_xpath('.//div[contains(@class, \"title\")]').text\n",
    "                    job_description = driver.find_element_by_xpath('.//div[@class=\"jobDescriptionContent desc\"]').text\n",
    "                    collected_successfully = True\n",
    "                except:\n",
    "                    time.sleep(5)\n",
    "\n",
    "            try:\n",
    "                salary_estimate = driver.find_element_by_xpath('.//span[@class=\"gray salary\"]').text\n",
    "            except NoSuchElementException:\n",
    "                salary_estimate = -1 #You need to set a \"not found value. It's important.\"\n",
    "\n",
    "            try:\n",
    "                rating = driver.find_element_by_xpath('.//span[@class=\"rating\"]').text\n",
    "            except NoSuchElementException:\n",
    "                rating = -1 #You need to set a \"not found value. It's important.\"\n",
    "\n",
    "            #Printing for debugging\n",
    "            if verbose:\n",
    "                print(\"Job Title: {}\".format(job_title))\n",
    "                print(\"Salary Estimate: {}\".format(salary_estimate))\n",
    "                print(\"Job Description: {}\".format(job_description[:500]))\n",
    "                print(\"Rating: {}\".format(rating))\n",
    "                print(\"Company Name: {}\".format(company_name))\n",
    "                print(\"Location: {}\".format(location))\n",
    "\n",
    "            #Going to the Company tab...\n",
    "            #clicking on this:\n",
    "            #<div class=\"tab\" data-tab-type=\"overview\"><span>Company</span></div>\n",
    "            try:\n",
    "                driver.find_element_by_xpath('.//div[@class=\"tab\" and @data-tab-type=\"overview\"]').click()\n",
    "\n",
    "                try:\n",
    "                    #<div class=\"infoEntity\">\n",
    "                    #    <label>Headquarters</label>\n",
    "                    #    <span class=\"value\">San Francisco, CA</span>\n",
    "                    #</div>\n",
    "                    headquarters = driver.find_element_by_xpath('.//div[@class=\"infoEntity\"]//label[text()=\"Headquarters\"]//following-sibling::*').text\n",
    "                except NoSuchElementException:\n",
    "                    headquarters = -1\n",
    "\n",
    "                try:\n",
    "                    size = driver.find_element_by_xpath('.//div[@class=\"infoEntity\"]//label[text()=\"Size\"]//following-sibling::*').text\n",
    "                except NoSuchElementException:\n",
    "                    size = -1\n",
    "\n",
    "                try:\n",
    "                    founded = driver.find_element_by_xpath('.//div[@class=\"infoEntity\"]//label[text()=\"Founded\"]//following-sibling::*').text\n",
    "                except NoSuchElementException:\n",
    "                    founded = -1\n",
    "\n",
    "                try:\n",
    "                    type_of_ownership = driver.find_element_by_xpath('.//div[@class=\"infoEntity\"]//label[text()=\"Type\"]//following-sibling::*').text\n",
    "                except NoSuchElementException:\n",
    "                    type_of_ownership = -1\n",
    "\n",
    "                try:\n",
    "                    industry = driver.find_element_by_xpath('.//div[@class=\"infoEntity\"]//label[text()=\"Industry\"]//following-sibling::*').text\n",
    "                except NoSuchElementException:\n",
    "                    industry = -1\n",
    "\n",
    "                try:\n",
    "                    sector = driver.find_element_by_xpath('.//div[@class=\"infoEntity\"]//label[text()=\"Sector\"]//following-sibling::*').text\n",
    "                except NoSuchElementException:\n",
    "                    sector = -1\n",
    "\n",
    "                try:\n",
    "                    revenue = driver.find_element_by_xpath('.//div[@class=\"infoEntity\"]//label[text()=\"Revenue\"]//following-sibling::*').text\n",
    "                except NoSuchElementException:\n",
    "                    revenue = -1\n",
    "\n",
    "                try:\n",
    "                    competitors = driver.find_element_by_xpath('.//div[@class=\"infoEntity\"]//label[text()=\"Competitors\"]//following-sibling::*').text\n",
    "                except NoSuchElementException:\n",
    "                    competitors = -1\n",
    "\n",
    "            except NoSuchElementException:  #Rarely, some job postings do not have the \"Company\" tab.\n",
    "                headquarters = -1\n",
    "                size = -1\n",
    "                founded = -1\n",
    "                type_of_ownership = -1\n",
    "                industry = -1\n",
    "                sector = -1\n",
    "                revenue = -1\n",
    "                competitors = -1\n",
    "\n",
    "\n",
    "            if verbose:\n",
    "                print(\"Headquarters: {}\".format(headquarters))\n",
    "                print(\"Size: {}\".format(size))\n",
    "                print(\"Founded: {}\".format(founded))\n",
    "                print(\"Type of Ownership: {}\".format(type_of_ownership))\n",
    "                print(\"Industry: {}\".format(industry))\n",
    "                print(\"Sector: {}\".format(sector))\n",
    "                print(\"Revenue: {}\".format(revenue))\n",
    "                print(\"Competitors: {}\".format(competitors))\n",
    "                print(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\")\n",
    "\n",
    "            jobs.append({\"Job Title\" : job_title,\n",
    "            \"Salary Estimate\" : salary_estimate,\n",
    "            \"Job Description\" : job_description,\n",
    "            \"Rating\" : rating,\n",
    "            \"Company Name\" : company_name,\n",
    "            \"Location\" : location,\n",
    "            \"Headquarters\" : headquarters,\n",
    "            \"Size\" : size,\n",
    "            \"Founded\" : founded,\n",
    "            \"Type of ownership\" : type_of_ownership,\n",
    "            \"Industry\" : industry,\n",
    "            \"Sector\" : sector,\n",
    "            \"Revenue\" : revenue,\n",
    "            \"Competitors\" : competitors})\n",
    "            #add job to jobs\n",
    "\n",
    "\n",
    "        #Clicking on the \"next page\" button\n",
    "        try:\n",
    "            page = driver.find_element_by_xpath('.//div[@class=\"tbl fill padHorz margVert\"]').text\n",
    "            page = page.split()\n",
    "            if page[1]==page[3]:\n",
    "                break\n",
    "            driver.find_element_by_xpath('.//li[@class=\"next\"]//a').click()\n",
    "        except NoSuchElementException:\n",
    "            print(\"Scraping terminated before reaching target number of jobs. Needed {}, got {}.\".format(num_jobs, len(jobs)))\n",
    "            break\n",
    "\n",
    "    return pd.DataFrame(jobs).reset_index()  #This line converts the dictionary object into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "637e5acd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchDriverException",
     "evalue": "Message: Unable to obtain driver for chrome; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\PC\\OneDrive\\Desktop\\AI Projects\\job_scrape\\venv\\lib\\site-packages\\selenium\\webdriver\\common\\driver_finder.py:64\u001b[0m, in \u001b[0;36mDriverFinder._binary_paths\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Path(path)\u001b[38;5;241m.\u001b[39mis_file():\n\u001b[1;32m---> 64\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe path is not a valid file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_paths[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdriver_path\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m path\n",
      "\u001b[1;31mValueError\u001b[0m: The path is not a valid file: chromedriver",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mNoSuchDriverException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchromedriver\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# make sure chromedriver is in PATH or give full path e.g. \"C:/path/to/chromedriver.exe\"\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mget_jobs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata scientist\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUncleaned_DS_jobs.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[14], line 18\u001b[0m, in \u001b[0;36mget_jobs\u001b[1;34m(keyword, num_jobs, verbose, path, slp_time)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Uncomment the line below if you'd like to scrape without a new Chrome window every time.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# options.add_argument('headless')\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# FIX: Use Service instead of executable_path\u001b[39;00m\n\u001b[0;32m     17\u001b[0m service \u001b[38;5;241m=\u001b[39m Service(path)\n\u001b[1;32m---> 18\u001b[0m driver \u001b[38;5;241m=\u001b[39m \u001b[43mwebdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChrome\u001b[49m\u001b[43m(\u001b[49m\u001b[43mservice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m driver\u001b[38;5;241m.\u001b[39mset_window_size(\u001b[38;5;241m1120\u001b[39m, \u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m     22\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.glassdoor.com/Job/jobs.htm?suggestCount=0&suggestChosen=false&clickSource=searchBtn&typedKeyword=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mkeyword\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m&sc.keyword=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mkeyword\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m&locT=&locId=&jobType=\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\PC\\OneDrive\\Desktop\\AI Projects\\job_scrape\\venv\\lib\\site-packages\\selenium\\webdriver\\chrome\\webdriver.py:47\u001b[0m, in \u001b[0;36mWebDriver.__init__\u001b[1;34m(self, options, service, keep_alive)\u001b[0m\n\u001b[0;32m     44\u001b[0m service \u001b[38;5;241m=\u001b[39m service \u001b[38;5;28;01mif\u001b[39;00m service \u001b[38;5;28;01melse\u001b[39;00m Service()\n\u001b[0;32m     45\u001b[0m options \u001b[38;5;241m=\u001b[39m options \u001b[38;5;28;01mif\u001b[39;00m options \u001b[38;5;28;01melse\u001b[39;00m Options()\n\u001b[1;32m---> 47\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbrowser_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDesiredCapabilities\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCHROME\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbrowserName\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvendor_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgoog\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\PC\\OneDrive\\Desktop\\AI Projects\\job_scrape\\venv\\lib\\site-packages\\selenium\\webdriver\\chromium\\webdriver.py:53\u001b[0m, in \u001b[0;36mChromiumDriver.__init__\u001b[1;34m(self, browser_name, vendor_prefix, options, service, keep_alive)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice \u001b[38;5;241m=\u001b[39m service\n\u001b[0;32m     52\u001b[0m finder \u001b[38;5;241m=\u001b[39m DriverFinder(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice, options)\n\u001b[1;32m---> 53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mfinder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_browser_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     54\u001b[0m     options\u001b[38;5;241m.\u001b[39mbinary_location \u001b[38;5;241m=\u001b[39m finder\u001b[38;5;241m.\u001b[39mget_browser_path()\n\u001b[0;32m     55\u001b[0m     options\u001b[38;5;241m.\u001b[39mbrowser_version \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\PC\\OneDrive\\Desktop\\AI Projects\\job_scrape\\venv\\lib\\site-packages\\selenium\\webdriver\\common\\driver_finder.py:47\u001b[0m, in \u001b[0;36mDriverFinder.get_browser_path\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_browser_path\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_binary_paths\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrowser_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\PC\\OneDrive\\Desktop\\AI Projects\\job_scrape\\venv\\lib\\site-packages\\selenium\\webdriver\\common\\driver_finder.py:78\u001b[0m, in \u001b[0;36mDriverFinder._binary_paths\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m     77\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to obtain driver for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbrowser\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NoSuchDriverException(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_paths\n",
      "\u001b[1;31mNoSuchDriverException\u001b[0m: Message: Unable to obtain driver for chrome; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location\n"
     ]
    }
   ],
   "source": [
    "path = \"chromedriver\"  # make sure chromedriver is in PATH or give full path e.g. \"C:/path/to/chromedriver.exe\"\n",
    "df = get_jobs('data scientist', 1500, False, path, 4)\n",
    "df.to_csv('Uncleaned_DS_jobs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca6d6148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping terminated early. Needed 50, got 0.\n",
      "Scraping completed and saved to Uncleaned_DS_jobs.csv\n"
     ]
    }
   ],
   "source": [
    "from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_jobs(keyword, num_jobs, verbose, slp_time):\n",
    "    \"\"\"Gathers jobs as a dataframe, scraped from Glassdoor\"\"\"\n",
    "\n",
    "    # Initialize Chrome driver with webdriver_manager\n",
    "    options = webdriver.ChromeOptions()\n",
    "    # options.add_argument(\"headless\")  # Uncomment if you want headless mode\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    driver.set_window_size(1120, 1000)\n",
    "\n",
    "    url = f\"https://www.glassdoor.com/Job/jobs.htm?suggestCount=0&suggestChosen=false&clickSource=searchBtn&typedKeyword={keyword}&sc.keyword={keyword}&locT=&locId=&jobType=\"\n",
    "    driver.get(url)\n",
    "    jobs = []\n",
    "\n",
    "    while len(jobs) < num_jobs:\n",
    "        time.sleep(slp_time)\n",
    "\n",
    "        # Close sign-up/login popup if it appears\n",
    "        try:\n",
    "            driver.find_element(\"css selector\", '[alt=\"Close\"]').click()\n",
    "            print(\"Popup closed\")\n",
    "        except NoSuchElementException:\n",
    "            pass\n",
    "\n",
    "\n",
    "        time.sleep(0.1)\n",
    "\n",
    "        try:\n",
    "            driver.find_element(\"css selector\", '[alt=\"Close\"]').click()\n",
    "            print(\"Popup closed\")\n",
    "        except NoSuchElementException:\n",
    "            pass\n",
    "\n",
    "        # Loop through job listings\n",
    "        job_buttons = driver.find_elements(\"class name\", \"jl\")  # job listing buttons\n",
    "        for job_button in job_buttons:\n",
    "            print(f\"Progress: {len(jobs)}/{num_jobs}\")\n",
    "            if len(jobs) >= num_jobs:\n",
    "                break\n",
    "\n",
    "            try:\n",
    "                job_button.click()\n",
    "                time.sleep(1)\n",
    "                collected_successfully = False\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            while not collected_successfully:\n",
    "                try:\n",
    "                    company_name = driver.find_element(\"xpath\", './/div[@class=\"employerName\"]').text\n",
    "                    location = driver.find_element(\"xpath\", './/div[@class=\"location\"]').text\n",
    "                    job_title = driver.find_element(\"xpath\", './/div[contains(@class, \"title\")]').text\n",
    "                    job_description = driver.find_element(\"xpath\", './/div[@class=\"jobDescriptionContent desc\"]').text\n",
    "                    collected_successfully = True\n",
    "                except:\n",
    "                    time.sleep(5)\n",
    "\n",
    "            try:\n",
    "                salary_estimate = driver.find_element(\"xpath\", './/span[@class=\"gray salary\"]').text\n",
    "            except NoSuchElementException:\n",
    "                salary_estimate = -1\n",
    "\n",
    "            try:\n",
    "                rating = driver.find_element(\"xpath\", './/span[@class=\"rating\"]').text\n",
    "            except NoSuchElementException:\n",
    "                rating = -1\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Job Title: {job_title}\")\n",
    "                print(f\"Salary Estimate: {salary_estimate}\")\n",
    "                print(f\"Job Description: {job_description[:500]}\")\n",
    "                print(f\"Rating: {rating}\")\n",
    "                print(f\"Company Name: {company_name}\")\n",
    "                print(f\"Location: {location}\")\n",
    "\n",
    "            # Go to Company tab\n",
    "            try:\n",
    "                driver.find_element(\"xpath\", './/div[@class=\"tab\" and @data-tab-type=\"overview\"]').click()\n",
    "\n",
    "                try:\n",
    "                    headquarters = driver.find_element(\"xpath\", './/div[@class=\"infoEntity\"]//label[text()=\"Headquarters\"]//following-sibling::*').text\n",
    "                except NoSuchElementException:\n",
    "                    headquarters = -1\n",
    "\n",
    "                try:\n",
    "                    size = driver.find_element(\"xpath\", './/div[@class=\"infoEntity\"]//label[text()=\"Size\"]//following-sibling::*').text\n",
    "                except NoSuchElementException:\n",
    "                    size = -1\n",
    "\n",
    "                try:\n",
    "                    founded = driver.find_element(\"xpath\", './/div[@class=\"infoEntity\"]//label[text()=\"Founded\"]//following-sibling::*').text\n",
    "                except NoSuchElementException:\n",
    "                    founded = -1\n",
    "\n",
    "                try:\n",
    "                    type_of_ownership = driver.find_element(\"xpath\", './/div[@class=\"infoEntity\"]//label[text()=\"Type\"]//following-sibling::*').text\n",
    "                except NoSuchElementException:\n",
    "                    type_of_ownership = -1\n",
    "\n",
    "                try:\n",
    "                    industry = driver.find_element(\"xpath\", './/div[@class=\"infoEntity\"]//label[text()=\"Industry\"]//following-sibling::*').text\n",
    "                except NoSuchElementException:\n",
    "                    industry = -1\n",
    "\n",
    "                try:\n",
    "                    sector = driver.find_element(\"xpath\", './/div[@class=\"infoEntity\"]//label[text()=\"Sector\"]//following-sibling::*').text\n",
    "                except NoSuchElementException:\n",
    "                    sector = -1\n",
    "\n",
    "                try:\n",
    "                    revenue = driver.find_element(\"xpath\", './/div[@class=\"infoEntity\"]//label[text()=\"Revenue\"]//following-sibling::*').text\n",
    "                except NoSuchElementException:\n",
    "                    revenue = -1\n",
    "\n",
    "                try:\n",
    "                    competitors = driver.find_element(\"xpath\", './/div[@class=\"infoEntity\"]//label[text()=\"Competitors\"]//following-sibling::*').text\n",
    "                except NoSuchElementException:\n",
    "                    competitors = -1\n",
    "\n",
    "            except NoSuchElementException:\n",
    "                headquarters = size = founded = type_of_ownership = industry = sector = revenue = competitors = -1\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Headquarters: {headquarters}\")\n",
    "                print(f\"Size: {size}\")\n",
    "                print(f\"Founded: {founded}\")\n",
    "                print(f\"Type of Ownership: {type_of_ownership}\")\n",
    "                print(f\"Industry: {industry}\")\n",
    "                print(f\"Sector: {sector}\")\n",
    "                print(f\"Revenue: {revenue}\")\n",
    "                print(f\"Competitors: {competitors}\")\n",
    "                print(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\")\n",
    "\n",
    "            jobs.append({\n",
    "                \"Job Title\": job_title,\n",
    "                \"Salary Estimate\": salary_estimate,\n",
    "                \"Job Description\": job_description,\n",
    "                \"Rating\": rating,\n",
    "                \"Company Name\": company_name,\n",
    "                \"Location\": location,\n",
    "                \"Headquarters\": headquarters,\n",
    "                \"Size\": size,\n",
    "                \"Founded\": founded,\n",
    "                \"Type of ownership\": type_of_ownership,\n",
    "                \"Industry\": industry,\n",
    "                \"Sector\": sector,\n",
    "                \"Revenue\": revenue,\n",
    "                \"Competitors\": competitors\n",
    "            })\n",
    "\n",
    "        # Next page\n",
    "        try:\n",
    "            page = driver.find_element(\"xpath\", './/div[@class=\"tbl fill padHorz margVert\"]').text\n",
    "            page = page.split()\n",
    "            if page[1] == page[3]:\n",
    "                break\n",
    "            driver.find_element(\"xpath\", './/li[@class=\"next\"]//a').click()\n",
    "        except NoSuchElementException:\n",
    "            print(f\"Scraping terminated early. Needed {num_jobs}, got {len(jobs)}.\")\n",
    "            break\n",
    "\n",
    "    driver.quit()\n",
    "    return pd.DataFrame(jobs).reset_index()\n",
    "\n",
    "\n",
    "# ----------------- Run scraper -----------------\n",
    "df = get_jobs(\"data scientist\", 50, False, 4)  # Try small number first (50)\n",
    "df.to_csv(\"Uncleaned_DS_jobs.csv\", index=False)\n",
    "print(\"Scraping completed and saved to Uncleaned_DS_jobs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6990489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping terminated early. Needed 50, got 0.\n",
      "✅ Scraping completed and saved to Uncleaned_DS_jobs.csv\n"
     ]
    }
   ],
   "source": [
    "from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_jobs(keyword, num_jobs, verbose, slp_time):\n",
    "    \"\"\"Gathers jobs as a dataframe, scraped from Glassdoor\"\"\"\n",
    "\n",
    "    # Initialize Chrome driver with webdriver_manager\n",
    "    options = webdriver.ChromeOptions()\n",
    "    # options.add_argument(\"headless\")  # Uncomment if you want headless mode\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    driver.set_window_size(1120, 1000)\n",
    "\n",
    "    # Open Glassdoor search page\n",
    "    url = f\"https://www.glassdoor.com/Job/jobs.htm?sc.keyword={keyword}\"\n",
    "    driver.get(url)\n",
    "    jobs = []\n",
    "\n",
    "    while len(jobs) < num_jobs:\n",
    "        time.sleep(slp_time)\n",
    "\n",
    "        # Close signup/login popup if it appears\n",
    "        try:\n",
    "            driver.find_element(\"css selector\", '[alt=\"Close\"]').click()\n",
    "            print(\"Popup closed\")\n",
    "        except NoSuchElementException:\n",
    "            pass\n",
    "\n",
    "        # Find job cards\n",
    "        job_cards = driver.find_elements(\"css selector\", \"li.react-job-listing\")\n",
    "\n",
    "        for job_card in job_cards:\n",
    "            print(f\"Progress: {len(jobs)}/{num_jobs}\")\n",
    "            if len(jobs) >= num_jobs:\n",
    "                break\n",
    "\n",
    "            try:\n",
    "                job_card.click()\n",
    "                time.sleep(2)  # wait for details to load\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            # Extract job info\n",
    "            try:\n",
    "                job_title = driver.find_element(\"css selector\", \"div[data-test='jobTitle']\").text\n",
    "            except NoSuchElementException:\n",
    "                job_title = -1\n",
    "\n",
    "            try:\n",
    "                company_name = driver.find_element(\"css selector\", \"div[data-test='employerName']\").text\n",
    "            except NoSuchElementException:\n",
    "                company_name = -1\n",
    "\n",
    "            try:\n",
    "                location = driver.find_element(\"css selector\", \"div[data-test='location']\").text\n",
    "            except NoSuchElementException:\n",
    "                location = -1\n",
    "\n",
    "            try:\n",
    "                job_description = driver.find_element(\"css selector\", \"div.jobDescriptionContent\").text\n",
    "            except NoSuchElementException:\n",
    "                job_description = -1\n",
    "\n",
    "            try:\n",
    "                salary_estimate = driver.find_element(\"css selector\", \"span[data-test='detailSalary']\").text\n",
    "            except NoSuchElementException:\n",
    "                salary_estimate = -1\n",
    "\n",
    "            try:\n",
    "                rating = driver.find_element(\"css selector\", \"span[data-test='detailRating']\").text\n",
    "            except NoSuchElementException:\n",
    "                rating = -1\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Job Title: {job_title}\")\n",
    "                print(f\"Company: {company_name}\")\n",
    "                print(f\"Location: {location}\")\n",
    "                print(f\"Salary: {salary_estimate}\")\n",
    "                print(f\"Rating: {rating}\")\n",
    "                print(f\"Description (first 200 chars): {job_description[:200]}\")\n",
    "                print(\"-\" * 50)\n",
    "\n",
    "            jobs.append({\n",
    "                \"Job Title\": job_title,\n",
    "                \"Company Name\": company_name,\n",
    "                \"Location\": location,\n",
    "                \"Salary Estimate\": salary_estimate,\n",
    "                \"Rating\": rating,\n",
    "                \"Job Description\": job_description,\n",
    "            })\n",
    "\n",
    "        # Try to go to next page\n",
    "        try:\n",
    "            next_button = driver.find_element(\"css selector\", \"button[data-test='pagination-next']\")\n",
    "            if not next_button.is_enabled():\n",
    "                print(\"No more pages.\")\n",
    "                break\n",
    "            next_button.click()\n",
    "        except NoSuchElementException:\n",
    "            print(f\"Scraping terminated early. Needed {num_jobs}, got {len(jobs)}.\")\n",
    "            break\n",
    "\n",
    "    driver.quit()\n",
    "    return pd.DataFrame(jobs).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# ----------------- Run scraper -----------------\n",
    "df = get_jobs(\"data scientist\", 50, True, 4)  # start with 50 jobs\n",
    "df.to_csv(\"Uncleaned_DS_jobs.csv\", index=False)\n",
    "print(\"✅ Scraping completed and saved to Uncleaned_DS_jobs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27dc79c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Got 20 jobs, saved to Uncleaned_DS_jobs.csv\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "\n",
    "def get_jobs(keyword, num_jobs, verbose, slp_time):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    driver.set_window_size(1120, 1000)\n",
    "\n",
    "    url = f\"https://www.glassdoor.com/Job/jobs.htm?sc.keyword={keyword}\"\n",
    "    driver.get(url)\n",
    "    jobs = []\n",
    "\n",
    "    while len(jobs) < num_jobs:\n",
    "        try:\n",
    "            job_cards = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"div[data-test='job-card-wrapper']\"))\n",
    "            )\n",
    "        except TimeoutException:\n",
    "            print(\"⚠️ No job cards found. Possible block or structure change.\")\n",
    "            break\n",
    "\n",
    "        for job_card in job_cards:\n",
    "            if len(jobs) >= num_jobs:\n",
    "                break\n",
    "\n",
    "            try:\n",
    "                job_card.click()\n",
    "                time.sleep(2)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            # Safe finder\n",
    "            def safe_find(selector, by=By.CSS_SELECTOR):\n",
    "                try:\n",
    "                    return driver.find_element(by, selector).text\n",
    "                except NoSuchElementException:\n",
    "                    return -1\n",
    "\n",
    "            job_title = safe_find(\"a[data-test='job-title']\")\n",
    "            company_name = safe_find(\"span.EmployerProfile_compactEmployerName__9MGcV\")\n",
    "            location = safe_find(\"div[data-test='emp-location']\")\n",
    "            salary_estimate = safe_find(\"span[id^='job-salary']\")\n",
    "            rating = safe_find(\"span.rating-single-star_RatingText__5fdjN\")\n",
    "            job_description = safe_find(\"div.jobDescriptionContent\")\n",
    "\n",
    "            jobs.append({\n",
    "                \"Job Title\": job_title,\n",
    "                \"Company Name\": company_name,\n",
    "                \"Location\": location,\n",
    "                \"Salary Estimate\": salary_estimate,\n",
    "                \"Rating\": rating,\n",
    "                \"Job Description\": job_description,\n",
    "            })\n",
    "\n",
    "        # Go to next page\n",
    "        try:\n",
    "            next_button = driver.find_element(By.CSS_SELECTOR, \"button[data-test='pagination-next']\")\n",
    "            if next_button.is_enabled():\n",
    "                next_button.click()\n",
    "                time.sleep(slp_time)\n",
    "            else:\n",
    "                break\n",
    "        except NoSuchElementException:\n",
    "            break\n",
    "\n",
    "    driver.quit()\n",
    "    return pd.DataFrame(jobs)\n",
    "\n",
    "\n",
    "# ---------------- Run scraper ----------------\n",
    "df = get_jobs(\"data scientist\", 20, True, 4)\n",
    "df.to_csv(\"Uncleaned_DS_jobs.csv\", index=False)\n",
    "print(f\"✅ Got {len(df)} jobs, saved to Uncleaned_DS_jobs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "131c027e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Got 20 jobs, saved to Uncleaned_DS_jobs.csv\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "\n",
    "def get_jobs(keyword, num_jobs, verbose, slp_time):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    driver.set_window_size(1120, 1000)\n",
    "\n",
    "    url = f\"https://www.glassdoor.com/Job/jobs.htm?sc.keyword={keyword}\"\n",
    "    driver.get(url)\n",
    "    jobs = []\n",
    "\n",
    "    while len(jobs) < num_jobs:\n",
    "        try:\n",
    "            job_cards = WebDriverWait(driver, 10).until(\n",
    "                    EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"div.JobCard_jobCardWrapper__vX29z\"))\n",
    "                )\n",
    "\n",
    "        except TimeoutException:\n",
    "            print(\"⚠️ No job cards found. Possible block or structure change.\")\n",
    "            break\n",
    "\n",
    "        for job_card in job_cards:\n",
    "            if len(jobs) >= num_jobs:\n",
    "                break\n",
    "\n",
    "            try:\n",
    "                job_card.click()\n",
    "                time.sleep(2)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            # Safe text finder\n",
    "            def safe_find(selector, by=By.CSS_SELECTOR):\n",
    "                try:\n",
    "                    return driver.find_element(by, selector).text\n",
    "                except NoSuchElementException:\n",
    "                    return -1\n",
    "\n",
    "            # Safe attribute finder\n",
    "            def safe_attr(selector, attr, by=By.CSS_SELECTOR):\n",
    "                try:\n",
    "                    return driver.find_element(by, selector).get_attribute(attr)\n",
    "                except NoSuchElementException:\n",
    "                    return -1\n",
    "\n",
    "            job_title = safe_find(\"a[data-test='job-title']\")\n",
    "            job_url = safe_attr(\"a[data-test='job-title']\", \"href\")\n",
    "            company_name = safe_find(\"span.EmployerProfile_compactEmployerName__9MGcV\")\n",
    "            location = safe_find(\"div[data-test='emp-location']\")\n",
    "            job_description = safe_find(\"div.jobDescriptionContent\")\n",
    "\n",
    "            jobs.append({\n",
    "                \"Job Title\": job_title,\n",
    "                \"Job Location\": location,\n",
    "                \"Company Name\": company_name,\n",
    "                \"Job Description\": job_description,\n",
    "                \"Source URL\": job_url\n",
    "            })\n",
    "\n",
    "        # Go to next page\n",
    "        try:\n",
    "            next_button = driver.find_element(By.CSS_SELECTOR, \"button[data-test='pagination-next']\")\n",
    "            if next_button.is_enabled():\n",
    "                next_button.click()\n",
    "                time.sleep(slp_time)\n",
    "            else:\n",
    "                break\n",
    "        except NoSuchElementException:\n",
    "            break\n",
    "\n",
    "    driver.quit()\n",
    "    return pd.DataFrame(jobs)\n",
    "\n",
    "\n",
    "# ---------------- Run scraper ----------------\n",
    "df = get_jobs(\"data scientist\", 20, True, 4)\n",
    "df.to_csv(\"Uncleaned_DS_jobs.csv\", index=False, encoding=\"utf-8\")\n",
    "print(f\"✅ Got {len(df)} jobs, saved to Uncleaned_DS_jobs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "62e511bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Got 20 jobs, saved to Uncleaned_DS_jobs.csv\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "\n",
    "def get_jobs(keyword, num_jobs, slp_time=3):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    driver.set_window_size(1200, 1000)\n",
    "\n",
    "    url = f\"https://www.glassdoor.com/Job/jobs.htm?sc.keyword={keyword}\"\n",
    "    driver.get(url)\n",
    "    jobs = []\n",
    "\n",
    "    while len(jobs) < num_jobs:\n",
    "        try:\n",
    "            job_cards = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"div.JobCard_jobCardWrapper__vX29z\"))\n",
    "            )\n",
    "        except TimeoutException:\n",
    "            print(\"⚠️ No job cards found. Structure may have changed.\")\n",
    "            break\n",
    "\n",
    "        for card in job_cards:\n",
    "            if len(jobs) >= num_jobs:\n",
    "                break\n",
    "\n",
    "            try:\n",
    "                title_elem = card.find_element(By.CSS_SELECTOR, \"a[data-test='job-title']\")\n",
    "                job_title = title_elem.text\n",
    "                job_url = title_elem.get_attribute(\"href\")\n",
    "            except:\n",
    "                job_title, job_url = -1, -1\n",
    "\n",
    "            try:\n",
    "                company_name = card.find_element(By.CSS_SELECTOR, \"span.EmployerProfile_compactEmployerName__9MGcV\").text\n",
    "            except:\n",
    "                company_name = -1\n",
    "\n",
    "            try:\n",
    "                location = card.find_element(By.CSS_SELECTOR, \"div[data-test='emp-location']\").text\n",
    "            except:\n",
    "                location = -1\n",
    "\n",
    "            # Wait for job description in side panel\n",
    "            try:\n",
    "                card.click()\n",
    "                desc_elem = WebDriverWait(driver, 5).until(\n",
    "                    EC.presence_of_element_located((By.CSS_SELECTOR, \"div.jobDescriptionContent\"))\n",
    "                )\n",
    "                job_description = desc_elem.text\n",
    "            except:\n",
    "                job_description = -1\n",
    "\n",
    "            jobs.append({\n",
    "                \"Job Title\": job_title,\n",
    "                \"Job Location\": location,\n",
    "                \"Company Name\": company_name,\n",
    "                \"Job Description\": job_description,\n",
    "                \"Source URL\": job_url\n",
    "            })\n",
    "\n",
    "        # Try to go to next page\n",
    "        try:\n",
    "            next_button = driver.find_element(By.CSS_SELECTOR, \"button[data-test='pagination-next']\")\n",
    "            if next_button.is_enabled():\n",
    "                next_button.click()\n",
    "                time.sleep(slp_time)\n",
    "            else:\n",
    "                break\n",
    "        except NoSuchElementException:\n",
    "            break\n",
    "\n",
    "    driver.quit()\n",
    "    return pd.DataFrame(jobs)\n",
    "\n",
    "\n",
    "# ---------------- Run scraper ----------------\n",
    "df = get_jobs(\"data scientist\", 20, 3)\n",
    "df.to_csv(\"Uncleaned_DS_jobs.csv\", index=False, encoding=\"utf-8\")\n",
    "print(f\"✅ Got {len(df)} jobs, saved to Uncleaned_DS_jobs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "59b80520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Got 20 jobs, saved to Uncleaned_DS_jobs.csv\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "\n",
    "def get_jobs(keyword, num_jobs, slp_time=3):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    driver.set_window_size(1200, 1000)\n",
    "\n",
    "    url = f\"https://www.glassdoor.com/Job/jobs.htm?sc.keyword={keyword}\"\n",
    "    driver.get(url)\n",
    "    jobs = []\n",
    "\n",
    "    while len(jobs) < num_jobs:\n",
    "        try:\n",
    "            job_cards = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"div.JobCard_jobCardWrapper__vX29z\"))\n",
    "            )\n",
    "        except TimeoutException:\n",
    "            print(\"⚠️ No job cards found. Structure may have changed.\")\n",
    "            break\n",
    "\n",
    "        for card in job_cards:\n",
    "            if len(jobs) >= num_jobs:\n",
    "                break\n",
    "\n",
    "            try:\n",
    "                title_elem = card.find_element(By.CSS_SELECTOR, \"a[data-test='job-title']\")\n",
    "                job_title = title_elem.text\n",
    "                job_url = title_elem.get_attribute(\"href\")\n",
    "            except:\n",
    "                job_title, job_url = -1, -1\n",
    "\n",
    "            try:\n",
    "                company_name = card.find_element(By.CSS_SELECTOR, \"span.EmployerProfile_compactEmployerName__9MGcV\").text\n",
    "            except:\n",
    "                company_name = -1\n",
    "\n",
    "            try:\n",
    "                location = card.find_element(By.CSS_SELECTOR, \"div[data-test='emp-location']\").text\n",
    "            except:\n",
    "                location = -1\n",
    "\n",
    "            # Extract job description (side panel)\n",
    "            try:\n",
    "                card.click()\n",
    "                desc_elem = WebDriverWait(driver, 5).until(\n",
    "                    EC.presence_of_element_located((\n",
    "                        By.CSS_SELECTOR,\n",
    "                        \"div.JobDetails_jobDescription__uW_fK.JobDetails_showHidden__C_FOA\"\n",
    "                    ))\n",
    "                )\n",
    "                job_description = desc_elem.text.strip()\n",
    "            except:\n",
    "                # fallback if class changes\n",
    "                try:\n",
    "                    desc_elem = driver.find_element(By.CSS_SELECTOR, \"div.jobDescriptionContent\")\n",
    "                    job_description = desc_elem.text.strip()\n",
    "                except:\n",
    "                    job_description = -1\n",
    "\n",
    "            jobs.append({\n",
    "                \"Job Title\": job_title,\n",
    "                \"Job Location\": location,\n",
    "                \"Company Name\": company_name,\n",
    "                \"Job Description\": job_description,\n",
    "                \"Source URL\": job_url\n",
    "            })\n",
    "\n",
    "        # Try to go to next page\n",
    "        try:\n",
    "            next_button = driver.find_element(By.CSS_SELECTOR, \"button[data-test='pagination-next']\")\n",
    "            if next_button.is_enabled():\n",
    "                next_button.click()\n",
    "                time.sleep(slp_time)\n",
    "            else:\n",
    "                break\n",
    "        except NoSuchElementException:\n",
    "            break\n",
    "\n",
    "    driver.quit()\n",
    "    return pd.DataFrame(jobs)\n",
    "\n",
    "\n",
    "# ---------------- Run scraper ----------------\n",
    "df = get_jobs(\"data scientist\", 20, 3)\n",
    "df.to_csv(\"Uncleaned_DS_jobs.csv\", index=False, encoding=\"utf-8\")\n",
    "print(f\"✅ Got {len(df)} jobs, saved to Uncleaned_DS_jobs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1b2a2bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Got 20 jobs, saved to Uncleaned_DS_jobs.csv\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "\n",
    "def get_jobs(keyword, num_jobs, slp_time=3):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"--start-maximized\")\n",
    "\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    driver.set_window_size(1200, 1000)\n",
    "\n",
    "    url = f\"https://www.glassdoor.com/Job/jobs.htm?sc.keyword={keyword}\"\n",
    "    driver.get(url)\n",
    "    jobs = []\n",
    "\n",
    "    while len(jobs) < num_jobs:\n",
    "        try:\n",
    "            job_cards = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"div.JobCard_jobCardWrapper__vX29z\"))\n",
    "            )\n",
    "        except TimeoutException:\n",
    "            print(\"⚠️ No job cards found. Structure may have changed.\")\n",
    "            break\n",
    "\n",
    "        for card in job_cards:\n",
    "            if len(jobs) >= num_jobs:\n",
    "                break\n",
    "\n",
    "            # --- Job Title & URL ---\n",
    "            try:\n",
    "                title_elem = card.find_element(By.CSS_SELECTOR, \"a[data-test='job-title']\")\n",
    "                job_title = title_elem.text\n",
    "                job_url = title_elem.get_attribute(\"href\")\n",
    "            except:\n",
    "                job_title, job_url = -1, -1\n",
    "\n",
    "            # --- Company Name ---\n",
    "            try:\n",
    "                company_name = card.find_element(By.CSS_SELECTOR, \"span.EmployerProfile_compactEmployerName__9MGcV\").text\n",
    "            except:\n",
    "                company_name = -1\n",
    "\n",
    "            # --- Location ---\n",
    "            try:\n",
    "                location = card.find_element(By.CSS_SELECTOR, \"div[data-test='emp-location']\").text\n",
    "            except:\n",
    "                location = -1\n",
    "\n",
    "            # --- Job Description (side panel) ---\n",
    "            try:\n",
    "                driver.execute_script(\"arguments[0].click();\", card)\n",
    "\n",
    "                # Wait until panel loads this job\n",
    "                WebDriverWait(driver, 10).until(\n",
    "                    EC.text_to_be_present_in_element((By.CSS_SELECTOR, \"div[data-test='jobTitle']\"), job_title)\n",
    "                )\n",
    "\n",
    "                # Try primary description selector\n",
    "                desc_elem = WebDriverWait(driver, 10).until(\n",
    "                    EC.presence_of_element_located((\n",
    "                        By.CSS_SELECTOR,\n",
    "                        \"div.JobDetails_jobDescription__uW_fK.JobDetails_showHidden__C_FOA\"\n",
    "                    ))\n",
    "                )\n",
    "                job_description = desc_elem.text.strip()\n",
    "\n",
    "            except Exception:\n",
    "                # Fallback: old selector\n",
    "                try:\n",
    "                    desc_elem = driver.find_element(By.CSS_SELECTOR, \"div.jobDescriptionContent\")\n",
    "                    job_description = desc_elem.text.strip()\n",
    "                except:\n",
    "                    job_description = -1\n",
    "\n",
    "            jobs.append({\n",
    "                \"Job Title\": job_title,\n",
    "                \"Job Location\": location,\n",
    "                \"Company Name\": company_name,\n",
    "                \"Job Description\": job_description,\n",
    "                \"Source URL\": job_url\n",
    "            })\n",
    "\n",
    "        # --- Next page ---\n",
    "        try:\n",
    "            next_button = driver.find_element(By.CSS_SELECTOR, \"button[data-test='pagination-next']\")\n",
    "            if next_button.is_enabled():\n",
    "                next_button.click()\n",
    "                time.sleep(slp_time)\n",
    "            else:\n",
    "                break\n",
    "        except NoSuchElementException:\n",
    "            break\n",
    "\n",
    "    driver.quit()\n",
    "    return pd.DataFrame(jobs)\n",
    "\n",
    "\n",
    "# ---------------- Run scraper ----------------\n",
    "df = get_jobs(\"data scientist\", 20, 3)\n",
    "df.to_csv(\"Uncleaned_DS_jobs.csv\", index=False, encoding=\"utf-8\")\n",
    "print(f\"✅ Got {len(df)} jobs, saved to Uncleaned_DS_jobs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714c1867",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common import NoSuchElementException\n",
    "import csv\n",
    "\n",
    "# Set up a controllable Chrome instance\n",
    "driver = webdriver.Chrome(service=Service())\n",
    "\n",
    "# Open the target page in the browser\n",
    "driver.get(\"https://www.indeed.com/jobs?q=data+scientist&l=New+York%2C+NY&from=searchOnDesktopSerp\")\n",
    "\n",
    "# A data structure where to store the scraped job openings\n",
    "jobs = []\n",
    "\n",
    "# Select the job opening elements on the page\n",
    "jobs_container_element = driver.find_element(By.CSS_SELECTOR, \"#mosaic-provider-jobcards\")\n",
    "job_elements = jobs_container_element.find_elements(By.CSS_SELECTOR, \"[data-testid=\"slider_item\"]\")\n",
    "\n",
    "# Scrape each job opening on the page\n",
    "for job_element in job_elements:\n",
    "    title_element = job_element.find_element(By.CSS_SELECTOR, \"h2.jobTitle\")\n",
    "    title = title_element.text\n",
    "\n",
    "    url_element = title_element.find_element(By.CSS_SELECTOR, \"a\")\n",
    "    url = url_element.get_attribute(\"href\")\n",
    "\n",
    "    company_element =job_element.find_element(By.CSS_SELECTOR, \"[data-testid=\"company-name\"]\")\n",
    "    company = company_element.text\n",
    "\n",
    "    location_element = job_element.find_element(By.CSS_SELECTOR, \"[data-testid=\"text-location\"]\")\n",
    "    location = location_element.text\n",
    "\n",
    "    tags = []\n",
    "    tags_container_element = job_element.find_element(By.CSS_SELECTOR, \".jobMetaDataGroup\")\n",
    "    tag_elements = tags_container_element.find_elements(By.CSS_SELECTOR, \"[data-testid=\"attribute_snippet_testid\"]\")\n",
    "    for tag_element in tag_elements:\n",
    "        tag = tag_element.text\n",
    "        tags.append(tag)\n",
    "\n",
    "    # Check whether the \"Easy Apply\" element is on the page\n",
    "    try:\n",
    "        job_element.find_element(By.CSS_SELECTOR, \"[data-testid=\"indeedApply\"]\")\n",
    "        easily_apply = True\n",
    "    except NoSuchElementException:\n",
    "        easily_apply = False\n",
    "\n",
    "    description = []\n",
    "    description_container_element = job_element.find_element(By.CSS_SELECTOR, \"[role=\"presentation\"]\")\n",
    "    description_elements = description_container_element.find_elements(By.CSS_SELECTOR, \"ul li\")\n",
    "    for description_element in description_elements:\n",
    "        description_item_text = description_element.text\n",
    "        # Ignore empty description strings\n",
    "        if (description_item_text != \"\"):\n",
    "            description.append(description_item_text)\n",
    "\n",
    "    # Store the scraped data\n",
    "    job = {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"company\": company,\n",
    "        \"location\": location,\n",
    "        \"tags\": tags,\n",
    "        \"easily_apply\": easily_apply,\n",
    "        \"description\": description\n",
    "    }\n",
    "    jobs.append(job)\n",
    "\n",
    "# Export the scraped data to an output CSV file\n",
    "csv_file = \"jobs.csv\"\n",
    "csv_headers = [\"title\", \"url\", \"company\", \"location\", \"tags\", \"easily_apply\", \"description\"]\n",
    "\n",
    "with open(csv_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=csv_headers)\n",
    "    writer.writeheader()\n",
    "    for job in jobs:\n",
    "        writer.writerow({\n",
    "            \"title\": job[\"title\"],\n",
    "            \"url\": job[\"url\"],\n",
    "            \"company\": job[\"company\"],\n",
    "            \"location\": job[\"location\"],\n",
    "            \"tags\": \";\".join(job[\"tags\"]),\n",
    "            \"easily_apply\": \"Yes\" if job[\"easily_apply\"] else \"No\",\n",
    "            \"description\": \";\".join(job[\"description\"])\n",
    "        })\n",
    "\n",
    "# Close the web driver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb38d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common import NoSuchElementException\n",
    "import csv\n",
    "import time\n",
    "import urllib.parse\n",
    "\n",
    "\n",
    "def get_jobs(job_title, num_jobs=20, num_pages=1, location=\"New York, NY\"):\n",
    "    \"\"\"\n",
    "    Scrape jobs from Indeed.\n",
    "\n",
    "    job_title : str -> e.g. \"data scientist\"\n",
    "    num_jobs  : int -> target number of jobs to scrape\n",
    "    num_pages : int -> how many pages to scrape\n",
    "    location  : str -> location string (default: New York, NY)\n",
    "    \"\"\"\n",
    "\n",
    "    job_query = urllib.parse.quote_plus(job_title)\n",
    "    loc_query = urllib.parse.quote_plus(location)\n",
    "\n",
    "    driver = webdriver.Chrome(service=Service())\n",
    "    jobs = []\n",
    "    jobs_scraped = 0\n",
    "    page = 0\n",
    "\n",
    "    while jobs_scraped < num_jobs and page < num_pages:\n",
    "        url = f\"https://www.indeed.com/jobs?q={job_query}&l={loc_query}&start={page*10}\"\n",
    "        driver.get(url)\n",
    "        time.sleep(2)\n",
    "\n",
    "        try:\n",
    "            jobs_container_element = driver.find_element(By.CSS_SELECTOR, \"#mosaic-provider-jobcards\")\n",
    "            job_elements = jobs_container_element.find_elements(By.CSS_SELECTOR, \"[data-testid='slider_item']\")\n",
    "        except Exception:\n",
    "            print(f\"No job container found on page {page+1}\")\n",
    "            break\n",
    "\n",
    "        for job_element in job_elements:\n",
    "            if jobs_scraped >= num_jobs:\n",
    "                break\n",
    "\n",
    "            try:\n",
    "                # Title & URL\n",
    "                title_element = job_element.find_element(By.CSS_SELECTOR, \"h2.jobTitle a\")\n",
    "                title = title_element.text\n",
    "                job_url = title_element.get_attribute(\"href\")\n",
    "\n",
    "                # Company\n",
    "                company_element = job_element.find_element(By.CSS_SELECTOR, \"[data-testid='company-name']\")\n",
    "                company = company_element.text\n",
    "\n",
    "                # Location\n",
    "                location_element = job_element.find_element(By.CSS_SELECTOR, \"[data-testid='text-location']\")\n",
    "                job_location = location_element.text\n",
    "\n",
    "                # Description\n",
    "                description = []\n",
    "                try:\n",
    "                    desc_container = job_element.find_element(By.CSS_SELECTOR, \"[role='presentation']\")\n",
    "                    desc_elements = desc_container.find_elements(By.CSS_SELECTOR, \"ul li\")\n",
    "                    description = [d.text for d in desc_elements if d.text.strip() != \"\"]\n",
    "                except NoSuchElementException:\n",
    "                    pass\n",
    "\n",
    "                job = {\n",
    "                    \"Job Title\": title,\n",
    "                    \"Job Location\": job_location,\n",
    "                    \"Company Name\": company,\n",
    "                    \"Job Description\": \" \".join(description),\n",
    "                    \"Source URL\": job_url\n",
    "                }\n",
    "                jobs.append(job)\n",
    "                jobs_scraped += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error scraping a job: {e}\")\n",
    "                continue\n",
    "\n",
    "        page += 1\n",
    "\n",
    "    # Save CSV\n",
    "    csv_file = \"jobs.csv\"\n",
    "    csv_headers = [\"Job Title\", \"Job Location\", \"Company Name\", \"Job Description\", \"Source URL\"]\n",
    "\n",
    "    with open(csv_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=csv_headers)\n",
    "        writer.writeheader()\n",
    "        for job in jobs:\n",
    "            writer.writerow(job)\n",
    "\n",
    "    driver.quit()\n",
    "    return jobs\n",
    "\n",
    "\n",
    "# ---------------- Run scraper ----------------\n",
    "if __name__ == \"__main__\":\n",
    "    jobs = get_jobs(\"data scientist\", num_jobs=20, num_pages=3, location=\"New York, NY\")\n",
    "    print(f\"Scraped {len(jobs)} jobs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc4aebc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431a3157",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2919ea91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
